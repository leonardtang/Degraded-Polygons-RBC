{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shapes dataset\n",
    "# Remove corners from each shape and save to new folder\n",
    "# Remove edges from each shape and save to new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "classes = [1, 3, 4, 5, 6, 7, 8]\n",
    "num_per_class = 2\n",
    "imgsize = 200\n",
    "min_radius = 20\n",
    "thickness = 2\n",
    "bg_color = (255, 255, 255)\n",
    "fg_color = (0, 0, 0)\n",
    "saveto = \"./images/shapes/\"\n",
    "corner_hide = 0.3 # Fraction of shape radius, such that a circle of bg_color will be drawn around each corner with this radius to hide the corner\n",
    "edge_hide = 0.3 # Fraction of shape radius, such that a circle of bg_color will be drawn around center of each edge with this radius to hide the edge\n",
    "\n",
    "for side in classes:\n",
    "    for k in range(num_per_class):\n",
    "        suffix = 'whole' # Save image with suffix to indicate corner_hide, edge_hide, or whole\n",
    "        # Create a blank image\n",
    "        img = np.zeros((imgsize, imgsize, 3), np.uint8)\n",
    "        img[:] = bg_color\n",
    "        # Randomly select a point on the image\n",
    "        x = np.random.randint(min_radius + math.ceil(thickness/2) + 1, imgsize-min_radius - math.ceil(thickness/2))\n",
    "        y = np.random.randint(min_radius + math.ceil(thickness/2) + 1, imgsize-min_radius - math.ceil(thickness/2))\n",
    "        # Get max radius of polygon\n",
    "        max_radius = min(x, y, imgsize - x, imgsize - y)\n",
    "        # Randomly select a radius\n",
    "        radius = np.random.randint(min_radius, max_radius)\n",
    "        # Randomly choose a starting angle\n",
    "        angle = np.random.randint(0, 360)\n",
    "        angle = angle * np.pi / 180 # Convert to radians\n",
    "        # Get list of points in polar coordinates\n",
    "        if side > 2:\n",
    "            angles = [angle + 2 * np.pi * i / side for i in range(side)]\n",
    "            points = np.array([(x + radius * np.cos(angle), y + radius * np.sin(angle)) for angle in angles], np.int32)\n",
    "            # Draw polygon\n",
    "            img = cv2.polylines(img, [points], True, fg_color, thickness)\n",
    "            # Save image\n",
    "            cv2.imwrite(f'{saveto}{side}_{k}_whole.png', img)\n",
    "            # Make copy of image with corners hidden\n",
    "            img_nocorners = img.copy()\n",
    "            for i in range(side):\n",
    "                cv2.circle(img_nocorners, (points[i][0], points[i][1]), math.ceil(corner_hide * radius), bg_color, -1)\n",
    "            # Save image\n",
    "            cv2.imwrite(f'{saveto}{side}_{k}_nocorners.png', img_nocorners)\n",
    "            # Make copy of image with edges hidden\n",
    "            img_noedges = img.copy()\n",
    "            for i in range(side):\n",
    "                cv2.circle(img_noedges, (int((points[i][0] + points[(i + 1) % side][0]) / 2), int((points[i][1] + points[(i + 1) % side][1]) / 2)), math.ceil(edge_hide * radius), bg_color, -1)\n",
    "            # Save image\n",
    "            cv2.imwrite(f'{saveto}{side}_{k}_noedges.png', img_noedges)\n",
    "        elif side == 1:\n",
    "            # Draw circle\n",
    "            img = cv2.circle(img, (x, y), radius, fg_color, thickness)\n",
    "            # Save image\n",
    "            cv2.imwrite(f'{saveto}{side}_{k}_whole.png', img)\n",
    "        else:\n",
    "            print(\"Invalid side number\")\n",
    "\n",
    "# Generate metadata df for whole shapes dataset\n",
    "for type in ['whole', 'nocorners', 'noedges']:\n",
    "    df = pd.DataFrame(columns=['filename', 'label'])\n",
    "    for path in os.listdir(f'./images/shapes/'):\n",
    "        if path.endswith(f'_{type}.png'):\n",
    "            df = df.append({'filename': path, 'label': path.split('_')[0]}, ignore_index=True)\n",
    "    df.to_csv('./images/shapes/train_metadata.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset from images\n",
    "class ShapesDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, metadata, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = pd.read_csv(metadata)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.istensor(index):\n",
    "            index = index.tolist()\n",
    "        \n",
    "        # Read images and targets\n",
    "        img_path = os.path.join(self.data_dir, self.metadata.iloc[index, 0])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.metadata.iloc[index, 1]\n",
    "\n",
    "        pair = {'image': image, 'label': label}\n",
    "\n",
    "        # Transform if requested\n",
    "        if self.transform:\n",
    "            pair = self.transform(pair)\n",
    "\n",
    "        return pair\n",
    "    \n",
    "def save_checkpoint(state, filename):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def cosine_annealing(step, total_steps, lr_max, lr_min): \n",
    "    return lr_min + (lr_max - lr_min) * 0.5 * (1 + np.cos(step / total_steps * np.pi))\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"\n",
    "    Computes the accuracy over the top k predictions for the specified values of k\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "\n",
    "        print(\"RES array\", res)\n",
    "        return res\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, scheduler, epoch, args):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        x = images.cuda()\n",
    "        y = target.cuda()\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        output, target = logits, y \n",
    "\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            progress.display(i)\n",
    "\n",
    "        print('Train * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "    \n",
    "    return losses.avg, top1.avg, top5.avg\n",
    "\n",
    "\n",
    "def val(test_loader, model, criterion, args):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(test_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: '\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        for i, (images, target) in enumerate(test_loader):\n",
    "            images = images.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - start)\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        print('Val * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Trains classifier on TU Berlin Sketch Dataset\", \n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\"--data\", \"-d\", type=str, default=\"./data/png\", choices=[\"./data/png\"])\n",
    "    parser.add_argument(\"--model\", \"-m\", type=str, default=\"vgg16\")\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=4)\n",
    "    parser.add_argument(\"--batch-size\", \"-b\", type=int, default=128)\n",
    "    parser.add_argument(\"--pretrained\", \"-p\", action=\"store_true\")\n",
    "    parser.add_argument(\"--print-freq\", \"-f\", type=int, default=10)\n",
    "    parser.add_argument(\"--gpu\", \"-g\", action=\"store_true\")\n",
    "    parser.add_argument(\"--epochs\", default=10, type=int)\n",
    "    parser.add_argument(\"--save-dir\", type=str, default=\"./checkpoints\")\n",
    "    # Hyperparams adapted from http://cs231n.stanford.edu/reports/2017/pdfs/420.pdf\n",
    "    parser.add_argument(\"--learning-rate\", \"-lr\", type=float, default=0.001, help=\"Initial learning rate.\")\n",
    "    parser.add_argument(\"--momentum\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--decay\", \"-wd\", type=float, default=0.01)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.data == \"./data/png\":\n",
    "        tub_train_transforms = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomRotation(25),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            # TODO: properly calculate image statistics\n",
    "            transforms.Normalize([0.5] * 3, [0.5] * 3) \n",
    "        ])\n",
    "        tub_test_transforms = transforms.Compose([\n",
    "            transforms.Resize(255), \n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            # TODO: properly calculate image statistics\n",
    "            transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "        ])\n",
    "\n",
    "        dataset = datasets.ImageFolder(args.data)\n",
    "        train_lazydata = LazySketchDataset(dataset, tub_train_transforms)\n",
    "        val_lazydata = LazySketchDataset(dataset, tub_test_transforms)\n",
    "        test_lazydata = LazySketchDataset(dataset, tub_test_transforms)\n",
    "\n",
    "        train_size = 0.8\n",
    "        num_train = len(dataset)\n",
    "        indices = np.random.permutation(list(range(num_train)))\n",
    "        split = int(np.floor(train_size * num_train))\n",
    "        val_split = int(np.floor((train_size + (1 - train_size) / 2) * num_train))\n",
    "        train_idx, val_idx, test_idx = indices[:split], indices[split:val_split], indices[val_split:]\n",
    "        \n",
    "        train_data = torch.utils.data.Subset(train_lazydata, indices=train_idx)\n",
    "        val_data = torch.utils.data.Subset(val_lazydata, indices=val_idx)\n",
    "        test_data = torch.utils.data.Subset(test_lazydata, indices=test_idx)\n",
    "\n",
    "    else:\n",
    "        raise Exception(f\"{args.data} is not a supported dataset\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data, \n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers, \n",
    "        shuffle=True,\n",
    "        # Unclear if we need drop_last, e.g. AugMix doesn't have this\n",
    "        drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_data, \n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers, \n",
    "        shuffle=True,\n",
    "        # Unclear if we need drop_last, e.g. AugMix doesn't have this\n",
    "        drop_last=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data, \n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers, \n",
    "        shuffle=True,\n",
    "        # Unclear if we need drop_last, e.g. AugMix doesn't have this\n",
    "        drop_last=True)\n",
    "\n",
    "    if args.model == \"vgg16\":\n",
    "        model = models.vgg16(pretrained=args.pretrained)\n",
    "        features = []\n",
    "        for feat in list(model.features):\n",
    "            features.append(feat)\n",
    "            if isinstance(feat, nn.Conv2d):\n",
    "                features.append(nn.Dropout(p=0.5, inplace=True))\n",
    "\n",
    "        model.features = nn.Sequential(*features)\n",
    "        print(model)\n",
    "    else:\n",
    "        raise Exception(f\"{args.model} is not a supported model\")\n",
    "\n",
    "\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        args.learning_rate,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.decay,\n",
    "        nesterov=True\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: cosine_annealing( \n",
    "            step,\n",
    "            args.epochs * len(train_loader),\n",
    "            1,  \n",
    "            1e-6 / args.learning_rate\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # MAIN TRAINING LOOP\n",
    "    best_acc1 = 0\n",
    "    for epoch in range(args.epochs):\n",
    "        train_losses_avg, train_top1_avg, train_top5_avg = train(train_loader, model, criterion, optimizer, scheduler, epoch, args)\n",
    "        val_losses_avg, val_top1_avg, val_top5_avg = val(val_loader, model, criterion, args)\n",
    "\n",
    "        with open(os.path.join(args.save_dir, \"training_log.csv\"), \"a+\") as f:\n",
    "            f.write('%03d,%0.5f,%0.5f,%0.5f,%0.5f,%0.5f,%0.5f\\n' % (\n",
    "                (epoch + 1),\n",
    "                train_losses_avg, train_top1_avg, train_top5_avg,\n",
    "                val_losses_avg, val_top1_avg, val_top5_avg\n",
    "            )\n",
    "        )\n",
    "\n",
    "        best_acc1 = max(val_top5_avg, best_acc1)\n",
    "        save_file = os.path.join(args.save_dir, \"final_model.pth\")\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': args.model,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc1': best_acc1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, filename=save_file)\n",
    "\n",
    "    best_cp = torch.load(save_file)\n",
    "    best_model = model.load_state_dict(best_cp[\"state_dict\"])\n",
    "    test_losses_avgs, test_top1_avg, test_top5_avg = val(test_loader, best_model, criterion, args)\n",
    "    print(f'Test * Acc@1 {test_top1_avg} Acc@5 {test_top5_avg}')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('_NAP': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79def0c0aeed0307f0b7b7c13e84f80aa4409d9b4e442ea2e585af93b299d2b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
