{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardtang/NAP/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faaTqQKHtiQq"
      },
      "outputs": [],
      "source": [
        "# Create shapes dataset\n",
        "# Remove corners from each shape and save to new folder\n",
        "# Remove edges from each shape and save to new folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eErpnppqtiQs"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import argparse\n",
        "from torchvision import datasets, models, transforms\n",
        "import pandas as pd\n",
        "# from meter_utils import AverageMeter, ProgressMeter\n",
        "import time\n",
        "\n",
        "classes = [3, 4, 5, 6, 7, 8]\n",
        "num_per_class = 1000\n",
        "imgsize = 200\n",
        "min_radius = 20\n",
        "thickness = 2\n",
        "bg_color = (255, 255, 255)\n",
        "fg_color = (0, 0, 0)\n",
        "saveto = \"./images/shapes/\"\n",
        "corner_hide = 0.3 # Fraction of shape radius, such that a circle of bg_color will be drawn around each corner with this radius to hide the corner\n",
        "edge_hide = 0.3 # Fraction of shape radius, such that a circle of bg_color will be drawn around center of each edge with this radius to hide the edge\n",
        "\n",
        "if not os.path.exists(saveto):\n",
        "    os.makedirs(saveto)\n",
        "\n",
        "for side in classes:\n",
        "    for k in range(num_per_class):\n",
        "        suffix = 'whole' # Save image with suffix to indicate corner_hide, edge_hide, or whole\n",
        "        # Create a blank image\n",
        "        img = np.zeros((imgsize, imgsize, 3), np.uint8)\n",
        "        img[:] = bg_color\n",
        "        # Randomly select a point on the image\n",
        "        x = np.random.randint(min_radius + math.ceil(thickness/2) + 1, imgsize-min_radius - math.ceil(thickness/2))\n",
        "        y = np.random.randint(min_radius + math.ceil(thickness/2) + 1, imgsize-min_radius - math.ceil(thickness/2))\n",
        "        # Get max radius of polygon\n",
        "        max_radius = min(x, y, imgsize - x, imgsize - y)\n",
        "        # Randomly select a radius\n",
        "        radius = np.random.randint(min_radius, max_radius)\n",
        "        # Randomly choose a starting angle\n",
        "        angle = np.random.randint(0, 360)\n",
        "        angle = angle * np.pi / 180 # Convert to radians\n",
        "        # Get list of points in polar coordinates\n",
        "        if side > 2:\n",
        "            angles = [angle + 2 * np.pi * i / side for i in range(side)]\n",
        "            points = np.array([(x + radius * np.cos(angle), y + radius * np.sin(angle)) for angle in angles], np.int32)\n",
        "            # Draw polygon\n",
        "            img = cv2.polylines(img, [points], True, fg_color, thickness)\n",
        "            # Save image\n",
        "            cv2.imwrite(f'{saveto}{side}_{k}_whole.png', img)\n",
        "            # Make copy of image with corners hidden\n",
        "            img_nocorners = img.copy()\n",
        "            for i in range(side):\n",
        "                cv2.circle(img_nocorners, (points[i][0], points[i][1]), math.ceil(corner_hide * radius), bg_color, -1)\n",
        "            # Save image\n",
        "            cv2.imwrite(f'{saveto}{side}_{k}_nocorners.png', img_nocorners)\n",
        "            # Make copy of image with edges hidden\n",
        "            img_noedges = img.copy()\n",
        "            for i in range(side):\n",
        "                cv2.circle(img_noedges, (int((points[i][0] + points[(i + 1) % side][0]) / 2), int((points[i][1] + points[(i + 1) % side][1]) / 2)), math.ceil(edge_hide * radius), bg_color, -1)\n",
        "            # Save image\n",
        "            cv2.imwrite(f'{saveto}{side}_{k}_noedges.png', img_noedges)\n",
        "        elif side == 1:\n",
        "            # Draw circle\n",
        "            img = cv2.circle(img, (x, y), radius, fg_color, thickness)\n",
        "            # Save image\n",
        "            cv2.imwrite(f'{saveto}{side}_{k}_whole.png', img)\n",
        "        else:\n",
        "            print(\"Invalid side number\")\n",
        "\n",
        "# Generate metadata df for whole shapes dataset\n",
        "for type in ['whole', 'nocorners', 'noedges']:\n",
        "    df = pd.DataFrame(columns=['filename', 'label'])\n",
        "    for path in os.listdir(saveto):\n",
        "        if path.endswith(f'_{type}.png'):\n",
        "            df = pd.concat([df, pd.DataFrame([[path, int(path.split('_')[0])]], columns=['filename', 'label'])])\n",
        "    df.to_csv(f'{saveto}metadata_{type}.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "44fW5L5cwuwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "id": "gE5Fc-v5RweT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-Y6AI2tNtiQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd226b44-7b7a-433d-9cd7-525c7d3d2b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): Dropout(p=0.5, inplace=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): Dropout(p=0.5, inplace=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): Dropout(p=0.5, inplace=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): Dropout(p=0.5, inplace=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): Dropout(p=0.5, inplace=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): Dropout(p=0.5, inplace=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): Dropout(p=0.5, inplace=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): Dropout(p=0.5, inplace=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): Dropout(p=0.5, inplace=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): Dropout(p=0.5, inplace=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): Dropout(p=0.5, inplace=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): Dropout(p=0.5, inplace=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): Dropout(p=0.5, inplace=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n",
            "RES array [tensor([0.], device='cuda:0'), tensor([0.], device='cuda:0')]\n",
            "Epoch: [0][ 0/37]\tTime  0.939 ( 0.939)\tData  0.796 ( 0.796)\tLoss 2.7145e+02 (2.7145e+02)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\n",
            "Train * Acc@1 0.000 Acc@5 0.000\n",
            "RES array [tensor([10.9375], device='cuda:0'), tensor([88.2812], device='cuda:0')]\n",
            "Train * Acc@1 5.469 Acc@5 44.141\n",
            "RES array [tensor([14.0625], device='cuda:0'), tensor([86.7188], device='cuda:0')]\n",
            "Train * Acc@1 8.333 Acc@5 58.333\n",
            "RES array [tensor([14.8438], device='cuda:0'), tensor([89.0625], device='cuda:0')]\n",
            "Train * Acc@1 9.961 Acc@5 66.016\n",
            "RES array [tensor([17.9688], device='cuda:0'), tensor([81.2500], device='cuda:0')]\n",
            "Train * Acc@1 11.562 Acc@5 69.062\n",
            "RES array [tensor([15.6250], device='cuda:0'), tensor([82.8125], device='cuda:0')]\n",
            "Train * Acc@1 12.240 Acc@5 71.354\n",
            "RES array [tensor([17.9688], device='cuda:0'), tensor([82.8125], device='cuda:0')]\n",
            "Train * Acc@1 13.058 Acc@5 72.991\n",
            "RES array [tensor([16.4062], device='cuda:0'), tensor([77.3438], device='cuda:0')]\n",
            "Train * Acc@1 13.477 Acc@5 73.535\n",
            "RES array [tensor([17.1875], device='cuda:0'), tensor([82.8125], device='cuda:0')]\n",
            "Train * Acc@1 13.889 Acc@5 74.566\n",
            "RES array [tensor([19.5312], device='cuda:0'), tensor([83.5938], device='cuda:0')]\n",
            "Train * Acc@1 14.453 Acc@5 75.469\n",
            "RES array [tensor([13.2812], device='cuda:0'), tensor([80.4688], device='cuda:0')]\n",
            "Epoch: [0][10/37]\tTime  4.100 ( 2.511)\tData  3.969 ( 2.379)\tLoss 4.8670e+00 (5.9821e+01)\tAcc@1  13.28 ( 14.35)\tAcc@5  80.47 ( 75.92)\n",
            "Train * Acc@1 14.347 Acc@5 75.923\n",
            "RES array [tensor([21.8750], device='cuda:0'), tensor([85.1562], device='cuda:0')]\n",
            "Train * Acc@1 14.974 Acc@5 76.693\n",
            "RES array [tensor([17.9688], device='cuda:0'), tensor([87.5000], device='cuda:0')]\n",
            "Train * Acc@1 15.204 Acc@5 77.524\n",
            "RES array [tensor([14.0625], device='cuda:0'), tensor([78.1250], device='cuda:0')]\n",
            "Train * Acc@1 15.123 Acc@5 77.567\n",
            "RES array [tensor([14.0625], device='cuda:0'), tensor([82.0312], device='cuda:0')]\n",
            "Train * Acc@1 15.052 Acc@5 77.865\n",
            "RES array [tensor([10.1562], device='cuda:0'), tensor([86.7188], device='cuda:0')]\n",
            "Train * Acc@1 14.746 Acc@5 78.418\n",
            "RES array [tensor([19.5312], device='cuda:0'), tensor([84.3750], device='cuda:0')]\n",
            "Train * Acc@1 15.028 Acc@5 78.768\n",
            "RES array [tensor([10.9375], device='cuda:0'), tensor([79.6875], device='cuda:0')]\n",
            "Train * Acc@1 14.800 Acc@5 78.819\n",
            "RES array [tensor([16.4062], device='cuda:0'), tensor([82.0312], device='cuda:0')]\n",
            "Train * Acc@1 14.885 Acc@5 78.988\n",
            "RES array [tensor([14.8438], device='cuda:0'), tensor([84.3750], device='cuda:0')]\n",
            "Train * Acc@1 14.883 Acc@5 79.258\n",
            "RES array [tensor([14.0625], device='cuda:0'), tensor([81.2500], device='cuda:0')]\n",
            "Epoch: [0][20/37]\tTime  7.267 ( 4.094)\tData  7.136 ( 3.962)\tLoss 4.5432e+00 (3.3416e+01)\tAcc@1  14.06 ( 14.84)\tAcc@5  81.25 ( 79.35)\n",
            "Train * Acc@1 14.844 Acc@5 79.353\n",
            "RES array [tensor([11.7188], device='cuda:0'), tensor([76.5625], device='cuda:0')]\n",
            "Train * Acc@1 14.702 Acc@5 79.226\n",
            "RES array [tensor([15.6250], device='cuda:0'), tensor([82.0312], device='cuda:0')]\n",
            "Train * Acc@1 14.742 Acc@5 79.348\n",
            "RES array [tensor([17.9688], device='cuda:0'), tensor([78.9062], device='cuda:0')]\n",
            "Train * Acc@1 14.876 Acc@5 79.329\n",
            "RES array [tensor([18.7500], device='cuda:0'), tensor([85.9375], device='cuda:0')]\n",
            "Train * Acc@1 15.031 Acc@5 79.594\n",
            "RES array [tensor([17.9688], device='cuda:0'), tensor([84.3750], device='cuda:0')]\n",
            "Train * Acc@1 15.144 Acc@5 79.778\n",
            "RES array [tensor([21.0938], device='cuda:0'), tensor([84.3750], device='cuda:0')]\n",
            "Train * Acc@1 15.365 Acc@5 79.948\n",
            "RES array [tensor([17.9688], device='cuda:0'), tensor([78.9062], device='cuda:0')]\n",
            "Train * Acc@1 15.458 Acc@5 79.911\n",
            "RES array [tensor([16.4062], device='cuda:0'), tensor([87.5000], device='cuda:0')]\n",
            "Train * Acc@1 15.490 Acc@5 80.172\n",
            "RES array [tensor([17.1875], device='cuda:0'), tensor([82.0312], device='cuda:0')]\n",
            "Train * Acc@1 15.547 Acc@5 80.234\n",
            "RES array [tensor([12.5000], device='cuda:0'), tensor([86.7188], device='cuda:0')]\n",
            "Epoch: [0][30/37]\tTime 10.518 ( 5.690)\tData 10.388 ( 5.559)\tLoss 3.8159e+00 (2.4015e+01)\tAcc@1  12.50 ( 15.45)\tAcc@5  86.72 ( 80.44)\n",
            "Train * Acc@1 15.449 Acc@5 80.444\n",
            "RES array [tensor([21.8750], device='cuda:0'), tensor([85.1562], device='cuda:0')]\n",
            "Train * Acc@1 15.649 Acc@5 80.591\n",
            "RES array [tensor([18.7500], device='cuda:0'), tensor([85.9375], device='cuda:0')]\n",
            "Train * Acc@1 15.743 Acc@5 80.753\n",
            "RES array [tensor([14.8438], device='cuda:0'), tensor([83.5938], device='cuda:0')]\n",
            "Train * Acc@1 15.717 Acc@5 80.836\n",
            "RES array [tensor([10.9375], device='cuda:0'), tensor([79.6875], device='cuda:0')]\n",
            "Train * Acc@1 15.580 Acc@5 80.804\n",
            "RES array [tensor([13.2812], device='cuda:0'), tensor([84.3750], device='cuda:0')]\n",
            "Train * Acc@1 15.516 Acc@5 80.903\n",
            "RES array [tensor([12.5000], device='cuda:0'), tensor([87.5000], device='cuda:0')]\n",
            "Train * Acc@1 15.435 Acc@5 81.081\n",
            "RES array [tensor([15.6250], device='cuda:0'), tensor([89.0625], device='cuda:0')]\n",
            "Test: [0/4]\tTime  0.853 ( 0.853)\tLoss 5.7401e+00 (5.7401e+00)\tAcc@1  15.62 ( 15.62)\tAcc@5  89.06 ( 89.06)\n",
            "RES array [tensor([18.7500], device='cuda:0'), tensor([85.1562], device='cuda:0')]\n",
            "RES array [tensor([18.7500], device='cuda:0'), tensor([85.1562], device='cuda:0')]\n",
            "RES array [tensor([10.9375], device='cuda:0'), tensor([72.6562], device='cuda:0')]\n",
            "Val * Acc@1 16.016 Acc@5 83.008\n",
            "RES array [tensor([12.5000], device='cuda:0'), tensor([84.3750], device='cuda:0')]\n",
            "Test: [0/4]\tTime  0.875 ( 0.875)\tLoss 5.9027e+00 (5.9027e+00)\tAcc@1  12.50 ( 12.50)\tAcc@5  84.38 ( 84.38)\n",
            "RES array [tensor([14.8438], device='cuda:0'), tensor([79.6875], device='cuda:0')]\n",
            "RES array [tensor([17.9688], device='cuda:0'), tensor([88.2812], device='cuda:0')]\n",
            "RES array [tensor([13.2812], device='cuda:0'), tensor([81.2500], device='cuda:0')]\n",
            "Val * Acc@1 14.648 Acc@5 83.398\n",
            "Test * Acc@1 14.6484375 Acc@5 83.3984375\n"
          ]
        }
      ],
      "source": [
        "# Create torch dataset from images\n",
        "class ShapesDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, metadata, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.metadata = pd.read_csv(os.path.join(data_dir, metadata))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        \n",
        "        # Read images and targets\n",
        "        img_path = os.path.join(self.data_dir, self.metadata.iloc[index, 0])\n",
        "        image = Image.open(img_path)\n",
        "        label = self.metadata.iloc[index, 1]\n",
        "        \n",
        "        # # Convert image to channels first\n",
        "        # image = np.transpose(image, (2, 0, 1))\n",
        "\n",
        "        # Transform if requested\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def cosine_annealing(step, total_steps, lr_max, lr_min): \n",
        "    return lr_min + (lr_max - lr_min) * 0.5 * (1 + np.cos(step / total_steps * np.pi))\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"\n",
        "    Computes the accuracy over the top k predictions for the specified values of k\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "\n",
        "        print(\"RES array\", res)\n",
        "        return res\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, scheduler, epoch, args):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    data_time = AverageMeter('Data', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    progress = ProgressMeter(\n",
        "        len(train_loader),\n",
        "        [batch_time, data_time, losses, top1, top5],\n",
        "        prefix=\"Epoch: [{}]\".format(epoch)\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - start)\n",
        "\n",
        "        x = images.cuda().float()\n",
        "        y = target.cuda()\n",
        "\n",
        "        # print(f'**********IMAGE: {x[0]}')\n",
        "        # print(f'**********TARGET: {y}')\n",
        "        # print(f'**********TARGET TYPE: {type(y)}')\n",
        "\n",
        "        logits = model(x)\n",
        "        # print(f'**********LOGITS: {logits}')\n",
        "        loss = criterion(logits, y)\n",
        "        output, target = logits, y \n",
        "\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        top1.update(acc1[0], images.size(0))\n",
        "        top5.update(acc5[0], images.size(0))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            progress.display(i)\n",
        "\n",
        "        print('Train * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
        "    \n",
        "    return losses.avg, top1.avg, top5.avg\n",
        "\n",
        "\n",
        "def val(test_loader, model, criterion, args):\n",
        "    batch_time = AverageMeter('Time', ':6.3f')\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    progress = ProgressMeter(\n",
        "        len(test_loader),\n",
        "        [batch_time, losses, top1, top5],\n",
        "        prefix='Test: '\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start = time.time()\n",
        "        for i, (images, target) in enumerate(test_loader):\n",
        "            images = images.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), images.size(0))\n",
        "            top1.update(acc1[0], images.size(0))\n",
        "            top5.update(acc5[0], images.size(0))\n",
        "\n",
        "            batch_time.update(time.time() - start)\n",
        "\n",
        "            if i % args.print_freq == 0:\n",
        "                progress.display(i)\n",
        "\n",
        "        print('Val * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
        "\n",
        "    return losses.avg, top1.avg, top5.avg\n",
        "\n",
        "\n",
        "def main():\n",
        "    # parser = argparse.ArgumentParser(\n",
        "    #     description=\"Trains classifier on shapes dataset\", \n",
        "    #     formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    # )\n",
        "    # parser.add_argument(\"--data\", \"-d\", type=str, default=\"./data/shapes\", choices=[\"./data/shapes\"])\n",
        "    # parser.add_argument(\"--model\", \"-m\", type=str, default=\"vgg16\")\n",
        "    # parser.add_argument(\"--num-workers\", type=int, default=2)\n",
        "    # parser.add_argument(\"--batch-size\", \"-b\", type=int, default=128)\n",
        "    # parser.add_argument(\"--pretrained\", \"-p\", action=\"store_true\")\n",
        "    # parser.add_argument(\"--print-freq\", \"-f\", type=int, default=10)\n",
        "    # # parser.add_argument(\"--gpu\", \"-g\", action=\"store_true\")\n",
        "    # parser.add_argument(\"--epochs\", default=10, type=int)\n",
        "    # parser.add_argument(\"--save-dir\", type=str, default=\"./checkpoints\")\n",
        "    # # Hyperparams adapted from http://cs231n.stanford.edu/reports/2017/pdfs/420.pdf\n",
        "    # parser.add_argument(\"--learning-rate\", \"-lr\", type=float, default=0.001, help=\"Initial learning rate.\")\n",
        "    # parser.add_argument(\"--momentum\", type=float, default=0.9)\n",
        "    # parser.add_argument(\"--decay\", \"-wd\", type=float, default=0.01)\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # Make args as a class for running in colab\n",
        "    class Args:\n",
        "        def __init__(self, data='./images/shapes', model='vgg16', num_workers=2, batch_size=128, pretrained=False, print_freq=10, epochs=1, save_dir='./checkpoints', learning_rate=0.001, momentum=0.9, decay=0.01):\n",
        "            self.data = data\n",
        "            self.model = model\n",
        "            self.num_workers = num_workers\n",
        "            self.batch_size = batch_size\n",
        "            self.pretrained = pretrained\n",
        "            self.print_freq = print_freq\n",
        "            self.epochs = epochs\n",
        "            self.save_dir = save_dir\n",
        "            self.learning_rate = learning_rate\n",
        "            self.momentum = momentum\n",
        "            self.decay = decay\n",
        "\n",
        "    args = Args()\n",
        "\n",
        "\n",
        "    if args.data == \"./images/shapes\":\n",
        "        tub_train_transforms = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224),\n",
        "            transforms.RandomRotation(25),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            # TODO: properly calculate image statistics\n",
        "            transforms.Normalize([0.5] * 3, [0.5] * 3) \n",
        "        ])\n",
        "        tub_test_transforms = transforms.Compose([\n",
        "            transforms.Resize(255), \n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            # TODO: properly calculate image statistics\n",
        "            transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
        "        ])\n",
        "\n",
        "        # Initialize train, val, test sets\n",
        "        train_data = ShapesDataset(metadata='metadata_whole.csv', data_dir = args.data, transform = tub_train_transforms)\n",
        "        val_data = ShapesDataset(metadata='metadata_whole.csv', data_dir = args.data, transform = tub_test_transforms)\n",
        "        test_data = ShapesDataset(metadata='metadata_whole.csv', data_dir = args.data, transform = tub_test_transforms)\n",
        "        \n",
        "        # Get train, val, test splits\n",
        "        train_size = 0.8\n",
        "        total_num_train = len(train_data.metadata)\n",
        "        indices = np.random.permutation(list(range(total_num_train)))\n",
        "        split = int(np.floor(train_size * total_num_train))\n",
        "        val_split = int(np.floor((train_size + (1 - train_size) / 2) * total_num_train))\n",
        "        train_idx, val_idx, test_idx = indices[:split], indices[split:val_split], indices[val_split:]\n",
        "        \n",
        "        # Subset train, val, test sets\n",
        "        train_data = torch.utils.data.Subset(train_data, indices=train_idx)\n",
        "        val_data = torch.utils.data.Subset(val_data, indices=val_idx)\n",
        "        test_data = torch.utils.data.Subset(test_data, indices=test_idx)\n",
        "\n",
        "    else:\n",
        "        raise Exception(f\"{args.data} is not a supported dataset\")\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_data, \n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers, \n",
        "        shuffle=True,\n",
        "        # Unclear if we need drop_last, e.g. AugMix doesn't have this\n",
        "        drop_last=True)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_data, \n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers, \n",
        "        shuffle=True,\n",
        "        # Unclear if we need drop_last, e.g. AugMix doesn't have this\n",
        "        drop_last=True)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_data, \n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.num_workers, \n",
        "        shuffle=True,\n",
        "        # Unclear if we need drop_last, e.g. AugMix doesn't have this\n",
        "        drop_last=True)\n",
        "\n",
        "    if args.model == \"vgg16\":\n",
        "        model = models.vgg16(pretrained=args.pretrained)\n",
        "        features = []\n",
        "        for feat in list(model.features):\n",
        "            features.append(feat)\n",
        "            if isinstance(feat, nn.Conv2d):\n",
        "                features.append(nn.Dropout(p=0.5, inplace=True))\n",
        "\n",
        "        model.features = nn.Sequential(*features)\n",
        "        print(model)\n",
        "    else:\n",
        "        raise Exception(f\"{args.model} is not a supported model\")\n",
        "\n",
        "\n",
        "    model = torch.nn.DataParallel(model).cuda()\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    optimizer = torch.optim.SGD(\n",
        "        model.parameters(),\n",
        "        args.learning_rate,\n",
        "        momentum=args.momentum,\n",
        "        weight_decay=args.decay,\n",
        "        nesterov=True\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "        optimizer,\n",
        "        lr_lambda=lambda step: cosine_annealing( \n",
        "            step,\n",
        "            args.epochs * len(train_loader),\n",
        "            1,  \n",
        "            1e-6 / args.learning_rate\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # MAIN TRAINING LOOP\n",
        "    best_acc1 = 0\n",
        "    for epoch in range(args.epochs):\n",
        "        train_losses_avg, train_top1_avg, train_top5_avg = train(train_loader, model, criterion, optimizer, scheduler, epoch, args)\n",
        "        val_losses_avg, val_top1_avg, val_top5_avg = val(val_loader, model, criterion, args)\n",
        "\n",
        "        if not os.path.exists(os.path.join(args.save_dir, \"training_log.csv\")):\n",
        "          os.makedirs(args.save_dir)\n",
        "          os.system('touch training_log.csv')\n",
        "          \n",
        "        with open(os.path.join(args.save_dir, \"training_log.csv\"), \"a+\") as f:\n",
        "            f.write('%03d,%0.5f,%0.5f,%0.5f,%0.5f,%0.5f,%0.5f\\n' % (\n",
        "                (epoch + 1),\n",
        "                train_losses_avg, train_top1_avg, train_top5_avg,\n",
        "                val_losses_avg, val_top1_avg, val_top5_avg\n",
        "            )\n",
        "        )\n",
        "\n",
        "        best_acc1 = max(val_top5_avg, best_acc1)\n",
        "        save_file = os.path.join(args.save_dir, \"final_model.pth\")\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'arch': args.model,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'best_acc1': best_acc1,\n",
        "            'optimizer' : optimizer.state_dict(),\n",
        "        }, filename=save_file)\n",
        "\n",
        "    best_cp = torch.load(save_file)\n",
        "    model.load_state_dict(best_cp[\"state_dict\"])\n",
        "    test_losses_avgs, test_top1_avg, test_top5_avg = val(test_loader, model, criterion, args)\n",
        "    print(f'Test * Acc@1 {test_top1_avg} Acc@5 {test_top5_avg}')\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 ('_NAP': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "79def0c0aeed0307f0b7b7c13e84f80aa4409d9b4e442ea2e585af93b299d2b2"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}